{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_banner():\n",
    "    print(' Iter   Nfev     Step       Objective    Norm of g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def output_iteration_info(k, nf, t, f, g):\n",
    "    print('{0:5d} {1:6d} {2:10e} {3:10e} {4:10e}'.format(k, nf, t, f, linalg.norm(g, np.inf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_final_results(x, f, g, nf, ng, nh, k):   \n",
    "    print('\\n')\n",
    "    print('          x:', x)\n",
    "    print('        fun:', f)\n",
    "    print('        jac:', g)\n",
    "    print('norm of jac:', linalg.norm(g, np.inf))\n",
    "    print('       nfev:', nf)\n",
    "    print('       ngev:', ng)\n",
    "    print('       nhev:', nh)\n",
    "    print('        nit:', k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: The Rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x):\n",
    "    \"\"\"\n",
    "    Two-variable Rosenbrock function\n",
    "    \"\"\"\n",
    "    return 100*(x[1]-x[0]**2)**2 + (1-x[0])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 1000)\n",
    "y = np.linspace(-5, 5, 1000)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = objective(np.vstack([X.ravel(), Y.ravel()])).reshape((1000,1000))\n",
    "plt.contour(X, Y, Z, np.arange(10)**5, cmap='RdGy')\n",
    "plt.colorbar();\n",
    "plt.text(1, 1, 'x', va='center', ha='center', color='red', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x):\n",
    "    \"\"\"\n",
    "    Derivative of two-variable Rosenbrock function\n",
    "    \"\"\"\n",
    "    return np.array([    \n",
    "        400 * (x[0]**2 - x[1]) * x[0] + 2*(x[0]-1),\n",
    "        200 * (x[1] - x[0]**2)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(x):\n",
    "    \"\"\"\n",
    "    Hessian of two-variable Rosenbrock function\n",
    "    \"\"\"\n",
    "    return np.array([\n",
    "        [2 - 400 * (x[1] - 3 * x[0]**2), -400 * x[0]],\n",
    "        [                   -400 * x[0],         200]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested initial iterates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([-1.2, -1.2])\n",
    "# x0 = np.array([-1, 0.8])\n",
    "# x0 = np.array([-1.2, 1])\n",
    "# x0 = np.array([0.4, 0.2]) # start in convex region near the solution (from L. Roberts, Oxford University)\n",
    "# x0 = np.array([-0.9, 1])  # start in nonconvex region (from L. Roberts, Oxford University)\n",
    "# x0 = np.array([-50, 40])  # start very far away (L. Roberts, Oxford University)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â The steepest descent method (See Tutorial 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent_AllInOne(objective, gradient, x0):\n",
    "    \"\"\"Implementation of the steepest descent method with a backtracking-Armino linesearch.\n",
    "    Adapted from steepdes.m, a matlab script which has been around on the internet for a while.\n",
    "    Written by Philip E. Gill (UCSD) and Walter Murray (Stanford) for pegadogic use.\n",
    "    \"\"\"\n",
    "\n",
    "    kmax = 100000\n",
    "    jmax = 20\n",
    "\n",
    "    dxmax = 1\n",
    "\n",
    "    c1 = .0001\n",
    "    c1 = 1/4\n",
    "    beta = .5\n",
    "    x = x0.astype(float)\n",
    "    f = objective(x); nf = 1\n",
    "    g = gradient(x); ng = 1\n",
    "    \n",
    "    k = 1\n",
    "    \n",
    "    output_banner()\n",
    "    \n",
    "    while ((linalg.norm(g, np.inf) > 1e-6) and (k <= kmax)):\n",
    "        d = -g\n",
    "        t = min(1, dxmax/linalg.norm(g, np.inf))\n",
    "        xnew = x + t * d\n",
    "        fnew = objective(xnew); nf += 1\n",
    "        j = 1\n",
    "        while ((fnew > f + t * c1 * np.inner(g,d)) and (j <= jmax)):\n",
    "            t = t * beta\n",
    "            xnew = x + t * d\n",
    "            fnew = objective(xnew); nf += 1\n",
    "            j  += 1\n",
    "        if j > jmax:\n",
    "            print('Armijo failed to make progress')\n",
    "            break\n",
    "\n",
    "        if (k%100 == 1): output_iteration_info(k, nf, t, f, g)\n",
    "\n",
    "        x = xnew\n",
    "        f = fnew\n",
    "        g = gradient(x); ng += 1\n",
    "        k += 1\n",
    "\n",
    "    if k > kmax:\n",
    "        print('Steepest descent failed to converge after maxiter iterations')\n",
    "\n",
    "    output_final_results(x, f, g, nf, ng, 0, k);\n",
    "    return x, f, g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steepest_descent_AllInOne(objective,gradient,x0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linesearches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo(obj, grad, x0, f0, g0, t0, d, nf, ng):\n",
    "    \n",
    "    \"\"\" Backtracking-Armijo linesearch \"\"\"\n",
    "\n",
    "    c1 = 1e-4 \n",
    "    \n",
    "    iterMax = 20\n",
    "    \n",
    "    gtd0 = np.inner(g0,d)\n",
    "    \n",
    "    if (gtd0 >= 0):\n",
    "        raise SystemExit(\"Armijo: Direction provided is not a descent direction.\")\n",
    "    \n",
    "    t = t0\n",
    "\n",
    "    for k in range(iterMax):\n",
    "        x = x0 + t*d\n",
    "        f = obj(x)\n",
    "        if (f < f0 + c1*t*gtd0):\n",
    "            g = grad(x)\n",
    "            return x, f, g, t, nf + k + 1, ng + 1\n",
    "        else:\n",
    "            t = t/2\n",
    "    \n",
    "    raise SystemExit(\"Armijo: Maximum Iterations exceeded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wolfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wolfe(obj, grad, x0, f0, g0, t0, d, nf, ng):\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    c1 = 1e-4\n",
    "    c1 = 1/4\n",
    "    c2 = 0.90\n",
    "    \n",
    "    iterMax = 100\n",
    "    \n",
    "    a  = 0\n",
    "    b  = np.inf\n",
    "    gtd0 = np.inner(g0,d)\n",
    "\n",
    "    if (gtd0 >= 0):\n",
    "        raise SystemExit(\"Wolfe: Direction not a descent direction.\")\n",
    "\n",
    "    t  = t0;\n",
    "\n",
    "    for k in range(iterMax):\n",
    "        x = x0 + t*d\n",
    "        f = obj(x); nf += 1\n",
    "        if (f > f0 + c1*t*gtd0):\n",
    "            b = t\n",
    "            t = (a+b)/2\n",
    "        else:\n",
    "            g = grad(x); ng += 1\n",
    "            if (np.inner(g,d) < c2*gtd0):\n",
    "                a = t\n",
    "                if (b == np.inf):\n",
    "                    t = 2*t\n",
    "                else:\n",
    "                    t = (a+b)/2\n",
    "            else:\n",
    "                return x, f, g, t, nf, ng\n",
    "    \n",
    "\n",
    "    raise SystemExit(\"WOLFE: Maximum Iterations exceeded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Quasi-Newton Matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HUpdate_H(H, s, y):\n",
    "    \n",
    "    \"\"\" Updates matrix H \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descent directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(objective, gradient, linesearch, x0):\n",
    "    \n",
    "    \"\"\"Steepest gradient descent.\"\"\"\n",
    "\n",
    "    maxiter = 20000\n",
    "\n",
    "    x = x0.astype(float)\n",
    "    f = objective(x); nf = 1\n",
    "    g = gradient(x); ng = 1\n",
    "    \n",
    "    dxmax = 1\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    output_banner()\n",
    "    \n",
    "    while ((linalg.norm(g, np.inf) > 1e-6) and (k < maxiter)):\n",
    "        t = min(1, dxmax/linalg.norm(g, np.inf))\n",
    "        d = -g\n",
    "        x, f, g, t, nf, ng = linesearch(objective, gradient, x, f, g, 1, d, nf, ng)\n",
    "        k += 1\n",
    "        if (k%100 == 1): output_iteration_info(k, nf, t, f, g)\n",
    "\n",
    "    output_final_results(x, f, g, nf, ng, 0, k);\n",
    "    return x, f, g, nf, ng, k;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steepest_descent(objective,gradient, wolfe, x0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(objective, gradient, hessian, x0):\n",
    "    \"\"\"Implementation of the Newton method.\"\"\"\n",
    "\n",
    "    maxiter = 500\n",
    "    sigma = 1e-4\n",
    "    beta = .5\n",
    "\n",
    "    x = x0.astype(float)\n",
    "    f = objective(x); nf = 1\n",
    "    g = gradient(x); ng = 1\n",
    "    h = hessian(x); nh = 1\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    output_banner()\n",
    "    \n",
    "    while ((linalg.norm(g, np.inf) > 1e-10) and (k <= maxiter)):\n",
    "        d = - linalg.solve(h,g)\n",
    "        t = 1\n",
    "        xnew = x + t * d\n",
    "        fnew = objective(xnew); nf += 1\n",
    "        j = 1\n",
    "        while ((fnew > f + t * sigma * np.inner(g,d)) and (j <= 15)):\n",
    "            t = t * beta\n",
    "            xnew = x + t * d\n",
    "            fnew = objective(xnew); nf += 1\n",
    "            j += 1\n",
    "        if j > 15:\n",
    "            print('Armijo failed to make progress')\n",
    "            break\n",
    "        x = xnew\n",
    "        f = fnew\n",
    "        g = gradient(x); ng += 1 \n",
    "        h = hessian(x); nh +=1\n",
    "        k += 1\n",
    "        output_iteration_info(k, nf, t, f, g)\n",
    "\n",
    "\n",
    "    if k > maxiter:\n",
    "        print('Newton method failed to converge after maxiter iterations')\n",
    "    output_final_results(x, f, g, nf, ng, nh, k)\n",
    "    return x, f, g, nf, ng, nh, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton(objective, gradient, hessian, x0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimize.minimize(objective, x0, method=\"Newton-CG\", jac=gradient, hess=hessian)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_newton(objective, gradient, hessian, x0):\n",
    "    \"\"\"Implementation of the Modified-Newton method with positive-definitess check.\n",
    "    \"\"\"\n",
    "\n",
    "    maxiter = 3000\n",
    "    sigma = 1e-4\n",
    "    armax = 50\n",
    "    \n",
    "    beta = 1e-3\n",
    "\n",
    "    x = x0.astype(float)\n",
    "    n = len(x)\n",
    "    f = objective(x); nf = 1\n",
    "    g = gradient(x); ng = 1\n",
    "    h = hessian(x); nh = 1\n",
    "    \n",
    "    k = 1\n",
    "    \n",
    "    output_banner()\n",
    "    \n",
    "    while ((np.linalg.norm(g, np.inf) > 1e-10) and (k <= maxiter)):\n",
    "        \n",
    "        hmin = min(np.diagonal(h)) \n",
    "\n",
    "        if hmin > 0:\n",
    "            tau = 0\n",
    "        else:\n",
    "            tau = - hmin + beta\n",
    "\n",
    "        count = 1\n",
    "        while count <= 10:\n",
    "            try:\n",
    "                c, low = linalg.cho_factor(h + tau * np.eye(n))\n",
    "                break\n",
    "            except:\n",
    "                tau = max(2 * tau, beta)\n",
    "            count +=1\n",
    "            \n",
    "        if count > 10:\n",
    "            print('Failed to obtain a positive definite modified Hessian.')\n",
    "            break\n",
    "                \n",
    "        d = - linalg.cho_solve((c, low), g)\n",
    "\n",
    "        t = 1\n",
    "        xnew = x + t * d\n",
    "        fnew = objective(xnew); nf += 1\n",
    "        j = 1\n",
    "        while ((fnew > f + t * sigma * np.inner(g,d)) and (j <= armax)):\n",
    "            t = t/2\n",
    "            xnew = x + t * d\n",
    "            fnew = objective(xnew); nf += 1\n",
    "            j += 1\n",
    "        if j > armax:\n",
    "            print('Armijo failed to make progress')\n",
    "            break\n",
    "\n",
    "        x = xnew\n",
    "        f = fnew\n",
    "        g = gradient(x); ng += 1 \n",
    "        h = hessian(x); nh +=1\n",
    "        output_iteration_info(k, nf, t, f, g)\n",
    "        k += 1\n",
    "\n",
    "    if k > maxiter:\n",
    "        print('Newton method failed to converge after maxiter iterations')\n",
    "    output_final_results(x, f, g, nf, ng, nh, k)\n",
    "    return x, f, g, nf, ng, nh, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_newton(objective,gradient,hessian,x0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.minimize(objective, x0, method=\"Newton-CG\", jac=gradient, hess=hessian)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGSWolfe(objective, gradient, x0):\n",
    "\n",
    "    maxIter = 5000;\n",
    "\n",
    "    eps = 1e-6;\n",
    "    \n",
    "    x = x0;   \n",
    "\n",
    "    f = objective(x); nf = 1\n",
    "    g = gradient(x); ng = 1\n",
    "\n",
    "    I = np.eye(len(x))\n",
    "    H = I\n",
    "\n",
    "    output_banner()\n",
    "    \n",
    "    k = 1\n",
    "\n",
    "    while ((linalg.norm(g, np.inf) > eps) and (k <= maxIter)):\n",
    "        d = - np.dot(H, g)\n",
    "        xnew, fnew, gnew, t, nf, ng = wolfe(objective,gradient,x,f,g,1,d,nf,ng);\n",
    "        s  = xnew - x\n",
    "        y  = gnew - g\n",
    "        r  = 1/np.dot(y,s)\n",
    "        H  = np.dot((I - r * np.outer(s,y)), np.dot(H, (I - r * np.outer(y,s)))) + r * np.outer(s,s)\n",
    "        x  = xnew\n",
    "        f  = fnew\n",
    "        g  = gnew\n",
    "        output_iteration_info(k, nf, t, f, g)\n",
    "        k += 1\n",
    "    \n",
    "    if k > maxIter:\n",
    "        print('BFGS method failed to converge after maxiter iterations')\n",
    "    output_final_results(x, f, g, nf, ng, 0, k)\n",
    "    return x, f, g, nf, ng, 0, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFGSWolfe(objective, gradient, x0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.minimize(objective, x0, method=\"BFGS\", jac=gradient) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimize.minimize(objective, x0, method=\"CG\", jac=gradient)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Rosenbrock function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_rosen(x):\n",
    "    \"\"\"The Rosenbrock function as per scipy documentation\"\"\"\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "\n",
    "def generalized_rosen_der(x):\n",
    "    xm = x[1:-1]\n",
    "    xm_m1 = x[:-2]\n",
    "    xm_p1 = x[2:]\n",
    "    der = np.zeros_like(x)\n",
    "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
    "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
    "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
    "    return der\n",
    "\n",
    "def generalized_rosen_hess(x):\n",
    "    x = np.asarray(x)\n",
    "    H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n",
    "    diagonal = np.zeros_like(x)\n",
    "    diagonal[0] = 1200*x[0]**2-400*x[1]+2\n",
    "    diagonal[-1] = 200\n",
    "    diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n",
    "    H = H + np.diag(diagonal)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "x0 = 100*np.random.rand(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_newton(generalized_rosen, generalized_rosen_der, generalized_rosen_hess, x0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFGSWolfe(generalized_rosen, generalized_rosen_der, x0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = optimize.minimize(generalized_rosen, x0, method='BFGS', jac=generalized_rosen_der, options={'gtol': 1e-8, 'disp': True});\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
