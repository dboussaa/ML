{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to implement algorithms for unconstrained optimization. \n",
    "\n",
    "It is proposed to code \n",
    "1. the steepest descent method\n",
    "2. the Newton method\n",
    "3. the BFGS method\n",
    "\n",
    "Two linesearch strategies will be implemented, one based on the Armijo condition, and one on the Wolfe conditions.\n",
    "\n",
    "The programs are to be tested on the the Rosenbrock function:\n",
    "\n",
    "    f(x,y) = 100 (y - x^2)^2 + (1 - x)^2\n",
    "    \n",
    "Two initial guesses guesses are to be used: (1.2, 1.2) and (-1.2, 1).\n",
    "\n",
    "Additional test functions will be provided. Students are encouraged to experiments with these functions, and compare the results yielded by their routines with those given by the built-in routines (scipy.optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from scipy import optimize\n",
    "import sys"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Format output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_banner():\n",
    "    print(' Iter   Nfev     Step       Objective    Norm of g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def output_iteration_info(k, nf, t, f, g):\n",
    "    print('{0:5d} {1:6d} {2:10e} {3:10e} {4:10e}'.format(k, nf, t, f, LA.norm(g, np.inf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_final_results(x, f, g, nf, ng, nh, k):   \n",
    "    print('\\n')\n",
    "    print('          x:', x)\n",
    "    print('        fun:', f)\n",
    "    print('        jac:', g)\n",
    "    print('norm of jac:', LA.norm(g, np.inf))\n",
    "    print('       nfev:', nf)\n",
    "    print('       ngev:', ng)\n",
    "    print('       nhev:', nh)\n",
    "    print('        nit:', k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: The Rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x):\n",
    "    \"\"\"Rosen function\"\"\"\n",
    "    return 100*(x[1]-x[0]**2)**2 + (1-x[0])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 1000)\n",
    "y = np.linspace(-5, 5, 1000)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = objective(np.vstack([X.ravel(), Y.ravel()])).reshape((1000,1000))\n",
    "plt.contour(X, Y, Z, np.arange(10)**5, cmap='RdGy')\n",
    "plt.colorbar();\n",
    "plt.text(1, 1, 'x', va='center', ha='center', color='red', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x):\n",
    "    \"\"\"Derivative of Rosenbrock's function.\"\"\"\n",
    "    return np.array([    \n",
    "        400 * (x[0]**2 - x[1]) * x[0] + 2*(x[0]-1),\n",
    "        200 * (x[1] - x[0]**2)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(x):\n",
    "    \"\"\"Hessian of Rosenbrock's function.\"\"\"\n",
    "    return np.array([\n",
    "        [2 - 400 * (x[1] - 3 * x[0]**2), -400 * x[0]],\n",
    "        [                   -400 * x[0],         200]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â A first approach (See Tutorial 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent_AllInOne(objective, gradient, x0):\n",
    "    \"\"\"Implements simple gradient descent for the Rosen function.\"\"\"\n",
    "\n",
    "    maxiter = 20000\n",
    "\n",
    "    dxmax = 1\n",
    "\n",
    "    c1 = .0001\n",
    "    beta = .5\n",
    "    x = x0\n",
    "    f = objective(x); nf = 1\n",
    "    g = gradient(x); ng = 1\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    #\n",
    "    output_banner()\n",
    "    while ((LA.norm(g, np.inf) > 1e-6) and (k < maxiter)):\n",
    "        d = -g\n",
    "        t = min(1, dxmax/LA.norm(g, np.inf))\n",
    "        xnew = x + t * d\n",
    "        fnew = objective(xnew)\n",
    "        nf = nf + 1\n",
    "        j = 1\n",
    "        while ((fnew > f + t * c1 * np.inner(g,d)) and (j <= 15)):\n",
    "            t = t * beta\n",
    "            xnew = x + t * d\n",
    "            fnew = objective(xnew); nf += 1\n",
    "            j  += 1\n",
    "        if j > 15:\n",
    "            print('Armijo failed to make progress')\n",
    "            return\n",
    "        if (k%100 == 1):\n",
    "            print('{0:5d} {1:6d} {2:10e} {3:10e} {3:10e}'.format(k, nf, t, f, LA.norm(g, np.inf)))\n",
    "        x = xnew\n",
    "        f = fnew\n",
    "        g = gradient(x); ng += 1\n",
    "        k += 1\n",
    "\n",
    "    output_final_results(x, f, g, nf, ng, 0, k);\n",
    "    return x, f, g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steepest_descent_AllInOne(objective,gradient,[-1.2, -1.2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linesearches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo(obj, grad, x0, f0, g0, t0, d, nf, ng):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    c1 = 1e-4 \n",
    "    \n",
    "    iterMax = 20\n",
    "    \n",
    "    gtd0 = np.inner(g0,d)\n",
    "    \n",
    "    if (gtd0 >= 0):\n",
    "        print('ARMIJO: Direction provided is not a descent direction.')\n",
    "        sys.exec(1)\n",
    "    \n",
    "    t = t0\n",
    "\n",
    "    for k in range(iterMax):\n",
    "        x = x0 + t*d\n",
    "        f = obj(x)\n",
    "        if (f < f0 + c1*t*gtd0):\n",
    "            g = grad(x)\n",
    "            return x, f, g, t, nf + k + 1, ng + 1\n",
    "        else:\n",
    "            t = t/2\n",
    "    \n",
    "    print(\"ARMIJO: Maximum Iterations exceeded.\")\n",
    "    sys.exec(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wolfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wolfe(obj, grad, x0, f0, g0, t0, d, nf, ng):\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    c1 = 1e-4;\n",
    "    c2 = 0.90;\n",
    "    \n",
    "    iterMax = 20\n",
    "    \n",
    "    a  = 0\n",
    "    b  = np.inf\n",
    "    gtd0 = g0.T @ d\n",
    "\n",
    "    if (gtd0 >= 0):\n",
    "        print('WOLFE: Direction not a descent direction.')\n",
    "\n",
    "    t  = t0;\n",
    "\n",
    "    for k in range(iterMax):\n",
    "        x = x0 + t*d\n",
    "        f = obj(x); nf += 1\n",
    "        if (f > f0 + c1*t*gtd0):\n",
    "            b = t\n",
    "            t = (a+b)/2\n",
    "        elif (np.inner((g := grad(x)), d) < c2*gtd0):\n",
    "            print(np.inner((g := grad(x)),d) < c2*gtd0)\n",
    "            ng = ng + 1\n",
    "            a = t\n",
    "            if (b == np.inf):\n",
    "                t = 2*t\n",
    "            else:\n",
    "                t = (a+b)/2\n",
    "        else:\n",
    "            return x, f, grad(x), t, nf, ng+1\n",
    "\n",
    "    print(\"WOLFE: Maximum Iterations exceeded.\")\n",
    "    sys.exec(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descent directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(objective, gradient, linesearch, x0):\n",
    "    \n",
    "    \"\"\"Implements simple gradient descent for the Rosen function.\"\"\"\n",
    "\n",
    "    maxiter = 20000\n",
    "\n",
    "    #\n",
    "   \n",
    "    x = x0\n",
    "    f = objective(x); nf = 1\n",
    "    g = gradient(x); ng = 1\n",
    "    \n",
    "    dxmax = 1\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    output_banner()\n",
    "    while ((LA.norm(g, np.inf) > 1e-6) and (k < maxiter)):\n",
    "        t = min(1, dxmax/LA.norm(g, np.inf))\n",
    "        d = -g\n",
    "        x, f, g, t, nf, ng = armijo(objective, gradient, x, f, g, 1, d, nf, ng)\n",
    "        k += 1\n",
    "        if (k%100 == 1): output_iteration_info(k, nf, t, f, g)\n",
    "\n",
    "    output_final_results(x, f, g, nf, ng, 0, k);\n",
    "    return x, f, g, nf, ng, k;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steepest_descent(objective,gradient, wolfe, [-1.2, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(objective, gradient, hessian, x0):\n",
    "    \"\"\"Implements simple gradient descent for the Rosen function.\"\"\"\n",
    "\n",
    "    maxiter = 100\n",
    "    sigma = 1e-4\n",
    "    beta = .5\n",
    "\n",
    "    x = x0\n",
    "    f = objective(x); nf = 1\n",
    "    g = gradient(x); ng = 1\n",
    "    h = hessian(x); nh = 1\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    output_banner()\n",
    "    \n",
    "    while ((LA.norm(g, np.inf) > 1e-10) and (k <= maxiter)):\n",
    "        d = - LA.solve(h,g)\n",
    "        t = 1\n",
    "        xnew = x + t * d\n",
    "        fnew = objective(xnew); nf += 1\n",
    "        j = 1\n",
    "        while ((fnew > f + t * sigma * np.inner(g,d)) and (j <= 15)):\n",
    "            t = t * beta\n",
    "            xnew = x + t * d\n",
    "            fnew = objective(xnew); nf += 1\n",
    "            j += 1\n",
    "        if j > 15:\n",
    "            print('Armijo failed to make progress')\n",
    "            return\n",
    "        x = xnew\n",
    "        f = fnew\n",
    "        g = gradient(x); ng += 1 \n",
    "        h = hessian(x); nh +=1\n",
    "        k += 1\n",
    "        output_iteration_info(k, nf, t, f, g)\n",
    "\n",
    "\n",
    "    output_final_results(x, f, g, nf, ng, nh, k)\n",
    "    return x, f, g, nf, ng, nh, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton(objective,gradient,hessian,[-1.2, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimize.minimize(objective, [-1.2, 1], method=\"Newton-CG\", jac=gradient, hess=hessian)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGSWolfe(objective, gradient, x0):\n",
    "\n",
    "    maxIter = 500;\n",
    "\n",
    "    eps = 1e-6;\n",
    "    \n",
    "    x = x0;   \n",
    "\n",
    "    f = objective(x); nf = 1\n",
    "    g = gradient(x); ng = 1\n",
    "\n",
    "    I = np.eye(len(x))\n",
    "    H = I\n",
    "\n",
    "    output_banner()\n",
    "    \n",
    "    k = 1\n",
    "\n",
    "    while ((LA.norm(g, np.inf) > eps) and (k < maxIter)):\n",
    "        d = - np.dot(H, g)\n",
    "        xnew, fnew, gnew, t, nf, ng = wolfe(objective,gradient,x,f,g,1,d,nf,ng);\n",
    "        s  = xnew - x\n",
    "        y  = gnew - g\n",
    "        r  = 1/np.dot(y,s)\n",
    "        H  = np.dot((I - r * np.outer(s,y)), np.dot(H, (I - r * np.outer(y,s)))) + r * np.outer(s,s)\n",
    "        x  = xnew\n",
    "        f  = fnew\n",
    "        g  = gnew\n",
    "        k += 1\n",
    "        output_iteration_info(k, nf, t, f, g)\n",
    "    \n",
    "    output_final_results(x, f, g, nf, ng, 0, k)\n",
    "    return x, f, g, nf, ng, 0, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFGSWolfe(objective,gradient,[50, 50]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.minimize(objective, [50, 50], method=\"BFGS\", jac=gradient)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimize.minimize(objective, [12, 12], method=\"CG\", jac=gradient)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A second example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x[0]**4/4 - x[0]**2 + 2*x[0] + (x[1]-1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x):\n",
    "    return np.array([x[0]**3 - 2*x[0] + 2, 2*(x[1]-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(x):\n",
    "    return np.array([[3*x[0]**2-2,  0], [0, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further\n",
    "\n",
    "1. Test the developed functions on the generalized Rosenbrock function\n",
    "2. Code the conjugate-gradient algorithm\n",
    "2. Code a modified Newton direction algorithm to deal with the case in which the pure Newton direction is not a descent direction\n",
    "3. Code the stochastic gradient descent\n",
    "4. Experiment with other test functions (see provided file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
